{"cells":[{"cell_type":"markdown","source":["# CNN Visualization with [Grad-CAM](https://github.com/jacobgil/pytorch-grad-cam)"],"metadata":{"id":"jo40KDtpA5n8"},"id":"jo40KDtpA5n8"},{"cell_type":"markdown","id":"0176796f","metadata":{"id":"0176796f"},"source":["# Tutorial: Concept Activation Maps\n","## Adapting pixel attribution methods for embedding outputs from models\n","\n","In some cases deep learning models output a feature vector, also called sometimes an \"embedding\".\n","This embedding can then be compared to other embeddings to compute how similar they are.\n","\n","\n","A few examples of where this happens:\n","- In face recognition, where the model is trained to give similar feature representations to face images of the same person, and different representations to face images of different people.\n","\n","- In image retreival, where we want to retreive images that have similar embeddings.\n","\n","- In self supervised networks - often trained by creating different augmented views of the same image, and teaching the model to give close representations to these views.\n","\n","\n","\n","In this tutorial we will adapt pixel attribution methods for embedding networks (as apposed for just vanilla classification networks).\n","\n","We will have a reference embedding - our \"concept\".\n","\n","That question we want to answer is: \n","\n","**\"What in the image is similar, or different, than the concept embedding ?\"**\n","\n","\n","To achieve this We will create a target function that computes the similarity from the model output with a reference embedding."]},{"cell_type":"code","source":["!pip install grad-cam"],"metadata":{"id":"fC13po51AD0Q"},"id":"fC13po51AD0Q","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"7c145182","metadata":{"id":"7c145182"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","warnings.simplefilter('ignore')\n","from torchvision.models.segmentation import deeplabv3_resnet50\n","import torch\n","import torch.functional as F\n","import numpy as np\n","import requests\n","import cv2\n","import torchvision\n","from PIL import Image\n","from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n","from pytorch_grad_cam import GradCAM\n","\n","# A model wrapper that gets a resnet model and returns the features before the fully connected layer.\n","class ResnetFeatureExtractor(torch.nn.Module):\n","    def __init__(self, model):\n","        super(ResnetFeatureExtractor, self).__init__()\n","        self.model = model\n","        self.feature_extractor = torch.nn.Sequential(*list(self.model.children())[:-1])\n","                \n","    def __call__(self, x):\n","        return self.feature_extractor(x)[:, :, 0, 0]  # output shape of GAP is [B, D, 1, 1]\n","        \n","resnet = torchvision.models.resnet50(pretrained=True)\n","resnet.eval()\n","model = ResnetFeatureExtractor(resnet)\n","\n","\n","def get_image_from_url(url):\n","    \"\"\"A function that gets a URL of an image, \n","    and returns a numpy image and a preprocessed\n","    torch tensor ready to pass to the model \"\"\"\n","\n","    img = np.array(Image.open(requests.get(url, stream=True).raw))\n","    img = cv2.resize(img, (512, 512))\n","    rgb_img_float = np.float32(img) / 255\n","    input_tensor = preprocess_image(rgb_img_float,\n","                                   mean=[0.485, 0.456, 0.406],\n","                                   std=[0.229, 0.224, 0.225])\n","    return img, rgb_img_float, input_tensor"]},{"cell_type":"code","execution_count":null,"id":"6f301bc1","metadata":{"id":"6f301bc1"},"outputs":[],"source":["car_img, car_img_float, car_tensor = get_image_from_url(\"https://www.wallpapersin4k.org/wp-content/uploads/2017/04/Foreign-Cars-Wallpapers-4.jpg\")\n","cloud_img, cloud_img_float, cloud_tensor = get_image_from_url(\"https://th.bing.com/th/id/OIP.CmONj_pGCXg9Hq9-OxTD9gHaEo?pid=ImgDet&rs=1\")\n","car_concept_features = model(car_tensor)[0, :]\n","cloud_concept_features = model(cloud_tensor)[0, :]"]},{"cell_type":"code","source":["print(model.feature_extractor(car_tensor).shape)"],"metadata":{"id":"0FAXqsKqBUAh"},"id":"0FAXqsKqBUAh","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"ede738b0","metadata":{"id":"ede738b0"},"source":["Lets visualize the concepts we're going to be looking for in the image.\n","Notice that the model is pretrained on imagenet. \n","However these images don't correspond to any of the image-net categories - there are no \"sky\" or \"car\" categories in imagenet.\n","We're still going to be able to find these \"concepts\" in our query image later."]},{"cell_type":"code","execution_count":null,"id":"77468bba","metadata":{"id":"77468bba"},"outputs":[],"source":["Image.fromarray(np.hstack((cloud_img, car_img)))"]},{"cell_type":"markdown","id":"f93ef672","metadata":{"id":"f93ef672"},"source":["Now lets look at our query image, this is going to be the image that we're going to analyze and serch for the concepts inside it."]},{"cell_type":"code","execution_count":null,"id":"ace898b5","metadata":{"id":"ace898b5"},"outputs":[],"source":["image, image_float, input_tensor = get_image_from_url(\"https://th.bing.com/th/id/R.c65135374de94dea2e2bf8fe0a4818e7?rik=Z75HF5uFr56PAw&pid=ImgRaw&r=0\")\n","Image.fromarray(image)"]},{"cell_type":"markdown","id":"5efe3a0f","metadata":{"id":"5efe3a0f"},"source":["To use the CAM objects we need to define two things:\n","- **The target function**.\n","  \n","  For classification networks this is typically one of the model category outputs, \n","  \n","  e.g, `ClassifierOutputTarget(281)`.\n","  \n","  \n"," \n","    However here we the target to be the similarity (or dissimilarity) to a target embedding, \n","  \n","    so we will implement it for this use case.\n","      \n","- **The target layer:** \n","    Which layer with 2D spatial outputs in the model we want to use.\n","    \n","    This is typically the output of the last convolutional layer in the network.\n","    \n"," \n","Lets define the target function and the target layer, and create the visualizations:"]},{"cell_type":"code","execution_count":null,"id":"8d550218","metadata":{"id":"8d550218"},"outputs":[],"source":["class SimilarityToConceptTarget:\n","    def __init__(self, features):\n","        self.features = features\n","    \n","    def __call__(self, model_output):\n","        cos = torch.nn.CosineSimilarity(dim=0)\n","        return cos(model_output, self.features)\n","    \n","target_layers = [resnet.layer4[-1]]\n","car_targets = [SimilarityToConceptTarget(car_concept_features)]\n","cloud_targets = [SimilarityToConceptTarget(cloud_concept_features)]\n","\n","# Where is the car in the image\n","with GradCAM(model=model,\n","             target_layers=target_layers,\n","             use_cuda=False) as cam:\n","    car_grayscale_cam = cam(input_tensor=input_tensor,\n","                        targets=car_targets)[0, :]\n","car_cam_image = show_cam_on_image(image_float, car_grayscale_cam, use_rgb=True)\n","Image.fromarray(car_cam_image)"]},{"cell_type":"markdown","id":"975795ff","metadata":{"id":"975795ff"},"source":["As you can see, it highlights the car in the image.\n","Now lets do the same, but for the cloud concept:"]},{"cell_type":"code","execution_count":null,"id":"29278246","metadata":{"id":"29278246"},"outputs":[],"source":["# Where is the cloud in the image\n","with GradCAM(model=model,\n","             target_layers=target_layers,\n","             use_cuda=False) as cam:\n","    cloud_grayscale_cam = cam(input_tensor=input_tensor,\n","                        targets=cloud_targets)[0, :]\n","cloud_cam_image = show_cam_on_image(image_float, cloud_grayscale_cam, use_rgb=True)\n","Image.fromarray(cloud_cam_image)"]},{"cell_type":"markdown","id":"899796ea","metadata":{"id":"899796ea"},"source":["Similarly we can do the same to see where the concept is NOT in the image:"]},{"cell_type":"code","execution_count":null,"id":"9b8254c5","metadata":{"id":"9b8254c5"},"outputs":[],"source":["class DifferenceFromConceptTarget:\n","    def __init__(self, features):\n","        self.features = features\n","    \n","    def __call__(self, model_output):\n","        cos = torch.nn.CosineSimilarity(dim=0)\n","        return 1 - cos(model_output, self.features)\n","\n","not_car_targets = [DifferenceFromConceptTarget(car_concept_features)]\n","not_cloud_targets = [DifferenceFromConceptTarget(cloud_concept_features)]"]},{"cell_type":"code","execution_count":null,"id":"0a9fe061","metadata":{"id":"0a9fe061"},"outputs":[],"source":["# Where is the cloud not in the image?\n","with GradCAM(model=model,\n","             target_layers=target_layers,\n","             use_cuda=False) as cam:\n","    not_cloud_grayscale_cam = cam(input_tensor=input_tensor,\n","                        targets=not_cloud_targets)[0, :]\n","cam_image = show_cam_on_image(image_float, not_cloud_grayscale_cam, use_rgb=True)\n","Image.fromarray(cam_image)"]},{"cell_type":"code","execution_count":null,"id":"3644bd43","metadata":{"id":"3644bd43"},"outputs":[],"source":["# Where is the car not in the image?\n","with GradCAM(model=model,\n","             target_layers=target_layers,\n","             use_cuda=False) as cam:\n","    not_car_grayscale_cam = cam(input_tensor=input_tensor,\n","                        targets=not_car_targets)[0, :]\n","cam_image = show_cam_on_image(image_float, not_car_grayscale_cam, use_rgb=True)\n","Image.fromarray(cam_image)"]},{"cell_type":"code","execution_count":null,"id":"f2bf1d6a","metadata":{"id":"f2bf1d6a"},"outputs":[],"source":["Image.fromarray(np.hstack((cloud_img, car_img, image, car_cam_image, cloud_cam_image)))"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"COMP6013-Prac11.ipynb.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}